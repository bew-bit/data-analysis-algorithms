{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать аналогичное изученному дерево решений, использующее энтропию Шенонна в качестве критерия информативности и сравнить точность(accuracy, balanced_accuracy) достигаемую на используемых синтетических данных с точностью разобранной на уроке реализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сгенерируем данные\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def entropy(p):\n",
    "    return - p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "\n",
    "def gini(p):\n",
    "    return p * (1 - p) + (1 - p) * ( 1 - (1 - p) )\n",
    "\"\"\"\n",
    "class Tree:    \n",
    "    def __init__(self, max_depth = 3, max_features = None,\n",
    "                 minimum_gain = 1e-7, min_samples_split = 10):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.minimum_gain = minimum_gain\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_class = np.unique(y).shape[0]\n",
    "\n",
    "        if self.max_features is None or self.max_features > X.shape[1]:\n",
    "            self.max_features = X.shape[1]\n",
    "\n",
    "        self.feature_importance = np.zeros(X.shape[1])\n",
    "        self.tree = _create_decision_tree(X, y, self.max_depth,\n",
    "                                          self.minimum_gain, self.max_features,\n",
    "                                          self.min_samples_split, self.n_class,\n",
    "                                          self.feature_importance, X.shape[0])\n",
    "        self.feature_importance /= np.sum(self.feature_importance)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        pred = np.argmax(proba, axis = 1)\n",
    "        return pred\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        proba = np.empty((X.shape[0], self.n_class))\n",
    "        for i in range(X.shape[0]):\n",
    "            proba[i] = self._predict_row(X[i, :], self.tree)\n",
    "\n",
    "        return proba\n",
    "\n",
    "    def _predict_row(self, row, tree):\n",
    "        if tree['is_leaf']:\n",
    "            return tree['prob']\n",
    "        else:\n",
    "            if row[tree['split_col']] <= tree['threshold']:\n",
    "                return self._predict_row(row, tree['left'])\n",
    "            else:\n",
    "                return self._predict_row(row, tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_decision_tree(X, y, max_depth,\n",
    "                          minimum_gain, max_features,\n",
    "                          min_samples_split, n_class,\n",
    "                          feature_importance, n_row):\n",
    "    try:\n",
    "        assert max_depth > 0\n",
    "        assert X.shape[0] > min_samples_split\n",
    "        column, value, gain = _find_best_split(X, y, max_features)\n",
    "        assert gain > minimum_gain\n",
    "        feature_importance[column] += (X.shape[0] / n_row) * gain\n",
    "        \n",
    "        left_X, right_X, left_y, right_y = _split(X, y, column, value)\n",
    "        left_child = _create_decision_tree(left_X, left_y, max_depth - 1,\n",
    "                                           minimum_gain, max_features,\n",
    "                                           min_samples_split, n_class,\n",
    "                                           feature_importance, n_row)\n",
    "        right_child = _create_decision_tree(right_X, right_y, max_depth - 1,\n",
    "                                            minimum_gain, max_features,\n",
    "                                            min_samples_split, n_class,\n",
    "                                            feature_importance, n_row)\n",
    "    except AssertionError:\n",
    "        counts = np.bincount(y, minlength = n_class)\n",
    "        prob = counts / y.shape[0]\n",
    "        leaf = {'is_leaf': True, 'prob': prob}\n",
    "        return leaf\n",
    "\n",
    "    node = {'is_leaf': False,\n",
    "            'left': left_child,\n",
    "            'right': right_child,\n",
    "            'split_col': column,\n",
    "            'threshold': value}\n",
    "    return node\n",
    "\n",
    "\n",
    "def _find_best_split(X, y, max_features):\n",
    "    subset = np.random.choice(X.shape[1], max_features, replace = False)\n",
    "    max_col, max_val, max_gain = None, None, None\n",
    "    parent_entropy = _compute_entropy(y)\n",
    "\n",
    "    for column in subset:\n",
    "        split_values = _find_splits(X, column)\n",
    "        for value in split_values:\n",
    "            splits = _split(X, y, column, value, return_X = False)\n",
    "            gain = parent_entropy - _compute_splits_entropy(y, splits)\n",
    "\n",
    "            if max_gain is None or gain > max_gain:\n",
    "                max_col, max_val, max_gain = column, value, gain\n",
    "\n",
    "    return max_col, max_val, max_gain\n",
    "\n",
    "\n",
    "def _compute_entropy(split):\n",
    "    _, counts = np.unique(split, return_counts = True)\n",
    "    p = counts / split.shape[0]\n",
    "    entropy = -np.sum(p * np.log2(p))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def _find_splits(X, column):\n",
    "    X_unique = np.unique(X[:, column])\n",
    "    split_values = np.empty(X_unique.shape[0] - 1)\n",
    "    for i in range(1, X_unique.shape[0]):\n",
    "        average = (X_unique[i - 1] + X_unique[i]) / 2\n",
    "        split_values[i - 1] = average\n",
    "\n",
    "    return split_values\n",
    "\n",
    "\n",
    "def _compute_splits_entropy(y, splits):\n",
    "    splits_entropy = 0\n",
    "    for split in splits:\n",
    "        splits_entropy += (split.shape[0] / y.shape[0]) * _compute_entropy(split)\n",
    "\n",
    "    return splits_entropy\n",
    "\n",
    "\n",
    "def _split(X, y, column, value, return_X = True):\n",
    "    left_mask = X[:, column] <= value\n",
    "    right_mask = X[:, column] > value\n",
    "    left_y, right_y = y[left_mask], y[right_mask]\n",
    "\n",
    "    if not return_X:\n",
    "        return left_y, right_y\n",
    "    else:\n",
    "        left_X, right_X = X[left_mask], X[right_mask]\n",
    "        return left_X, right_X, left_y, right_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "точность: 0.973\n"
     ]
    }
   ],
   "source": [
    "tree = Tree()\n",
    "tree.fit(X, y)\n",
    "y_pred = tree.predict(X)\n",
    "print(f'точность: {round(accuracy_score(y, y_pred),3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
